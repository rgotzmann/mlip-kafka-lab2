# ðŸ“¦ Kafka + Docker Producer/Consumer Lab

## Overview

In this hands-on lab, you will work with **Apache Kafka**, a distributed streaming platform for large-scale, real-time data, and gain exposure to **Docker containers**.

You will:

- Use Docker to pull and run a local Kafka instance
- Establish a secure connection to a Kafka broker
- Produce and consume messages using Python
- Explore Kafka command-line tools (`kcat`) for inspection and monitoring

This lab prepares you for the upcoming **Kafka Streams group project**.  
The notebook `KafkaDemo.ipynb` provides a step-by-step guide for Python interactions.

---

## ðŸŽ¯ Learning Objectives

By the end of this lab, you will be able to:

- Explain **topics** and **offsets** and how they enable message continuity after consumer disconnects
- Configure and run a Python producer and consumer against a Kafka broker
- Use `kcat` (or an equivalent CLI) to inspect, manage, and monitor topics and messages

---

## âœ… Deliverables

### Connection Setup

If you deployed Kafka on a remote server, cloud cluster, or managed service, provide:

- SSH tunnel command used
- Brief explanation
- Screenshot showing an active tunnel

---

### Python Producer & Consumer

Implement producer and consumer modes by modifying the starter code.

Include:

- Final Python scripts/note
